{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A very simple and basic web scraping script. Feel free to\n",
    "use this as a source of inspiration, but, make sure to attribute\n",
    "it if you do so.\n",
    "\n",
    "This is by no means production code.\n",
    "\"\"\"\n",
    "# built-in imports\n",
    "import re\n",
    "#from types import NoneType\n",
    "import requests\n",
    "from json import dump\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# user packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "\n",
    "\n",
    "#number of pages for each suburb (maximum is 50, 999 properties)\n",
    "N_PAGES = range(1, 50 + 1) # update this to your liking\n",
    "\n",
    "#https://www.domain.com.au/rent/melbourne-region-vic/?sort=price-desc&page={1}\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "\n",
    "\n",
    "\n",
    "postcode = 3000\n",
    "while postcode <= 3996:\n",
    "\n",
    "    # begin code\n",
    "    url_links = []\n",
    "    property_metadata = defaultdict(dict)\n",
    "\n",
    "    # generate list of urls to visit\n",
    "    for page in N_PAGES:\n",
    "\n",
    "        #the full url \n",
    "        url = BASE_URL + f\"/rent/?postcode={postcode}&page={page}\"\n",
    "\n",
    "        #parse into a BeautifulSoup object\n",
    "        bs_object = BeautifulSoup(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "\n",
    "\n",
    "        #Keith's Note: Some results for a postcode may not display up to N_PAGES pages.\n",
    "        #in that case the findAll() applied to the results secion of bs_object would return\n",
    "        #nothing (NoneType object)\n",
    "\n",
    "        try: \n",
    "            index_links = bs_object \\\n",
    "                .find(\n",
    "                    \"ul\",\n",
    "                    {\"data-testid\": \"results\"}\n",
    "                ) \\\n",
    "                .findAll(\n",
    "                    \"a\",\n",
    "                    href=re.compile(f\"{BASE_URL}/*\") # the `*` denotes wildcard any\n",
    "                )\n",
    "\n",
    "        except AttributeError:\n",
    "            #Keith's Notes: there's no more pages to search, might as well to just break this for loop. \n",
    "\n",
    "            #suggestion: if we break from this for loop, and page = 1\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "        for link in index_links:\n",
    "            # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n",
    "\n",
    "    print(f\"url links obtained for postcode {postcode} page {page}\")\n",
    "\n",
    "    # for each url, scrape some basic metadata\n",
    "    for property_url in url_links[1:]:\n",
    "\n",
    "        bs_object = BeautifulSoup(requests.get(property_url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "        # looks for the header class to get property name\n",
    "        property_metadata[property_url]['name'] = bs_object \\\n",
    "            .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "            .text\n",
    "\n",
    "        # looks for the div containing a summary title for cost\n",
    "        property_metadata[property_url]['cost_text'] = bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "            .text\n",
    "\n",
    "        # extract coordinates from the hyperlink provided\n",
    "        # i'll let you figure out what this does :P\n",
    "        property_metadata[property_url]['coordinates'] = [\n",
    "            float(coord) for coord in re.findall(\n",
    "                r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "                bs_object \\\n",
    "                    .find(\n",
    "                        \"a\",\n",
    "                        {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                    ) \\\n",
    "                    .attrs['href']\n",
    "            )[0].split(',')\n",
    "        ]\n",
    "\n",
    "        \n",
    "        #Keith's note: DEBUGGING INDEX ERROR: try to ignore the IndexError, see if any other error pops up\n",
    "        rooms_list = []\n",
    "        for feature in bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\"span\", {\"data-testid\": \"property-features-text-container\"}):\n",
    "            try:\n",
    "                rooms_list.append(re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0])\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        property_metadata[property_url]['rooms'] = rooms_list\n",
    "        \n",
    "        #Keith's note: below is the code that was actually given but replaced by above\n",
    "        # property_metadata[property_url]['rooms'] = [\n",
    "        #     re.findall(r'\\d\\s[A-Za-z]+', feature.text)[0] for feature in bs_object \\\n",
    "        #         .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "        #         .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "        # ]\n",
    "\n",
    "        property_metadata[property_url]['desc'] = re \\\n",
    "            .sub(r'<br\\/>', '\\n', str(bs_object.find(\"p\"))) \\\n",
    "            .strip('</p>')\n",
    "\n",
    "    print(f\"postcode {postcode} done, saving in progress..\")\n",
    "\n",
    "    # output to example json in data/raw/\n",
    "    with open(f'../data/raw/vic_properties_postcode_{postcode}.json', 'w') as f:\n",
    "        dump(property_metadata, f)\n",
    "\n",
    "    postcode += 1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the JSONs (the properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os\n",
    "\n",
    "individual_files = glob.glob('../data/raw/*.json')\n",
    "\n",
    "for i in individual_files:\n",
    "                      \n",
    "    with open(i, \"r\") as f: \n",
    "        if pd.read_json(i, lines = True).shape == (1,0):\n",
    "            os.remove(i)\n",
    "            continue\n",
    "            \n",
    "individual_files_new =  glob.glob('../data/raw/*.json')                      \n",
    "merged = pd.DataFrame(columns = ['coordinates', 'cost_text', 'desc', 'name', 'rooms'])\n",
    "                      \n",
    "for i in individual_files_new:                    \n",
    "    curr = pd.read_json(i).transpose()[['coordinates', 'cost_text', 'desc', 'name', 'rooms']]\n",
    "    merged = pd.concat([merged, curr])\n",
    "    \n",
    "merged.to_csv('../data/raw/All_Houses_Scraped.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
